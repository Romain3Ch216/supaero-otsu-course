{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"4af70523-7851-4e5f-8248-87e0cfb79d55","cell_type":"markdown","source":"# OTSU Big Data Cloud BE\n\nWelcome to this training of ISAE Supaero OTSU Cloud, Big Data and Machine Learning module. \n\nThe end goal of this exercise is to build temporal statistics from several Seninel-2 L2A products accessed from an object storage. In order to do this, we'll read the products using (rio)Xarray with a Dask backend, directly from Google Cloud Storage, compute a NDSI over ten dates and plot snow cover evolution accross the period.\n\nBy doing this, we will learn the folowing things:\n\n- How to access a Cloud storage and browse its objects,\n- How to use rioxarray to access and load Satellite imagery,\n- How to use Xarray Dataset class to build a complete timeseries dataset,\n- How to chunk our data and use Dask to perform bigger than memory or distributed analysis over our entire dataset.","metadata":{}},{"id":"8fff6544-8687-4441-8dec-05c80f3a31a2","cell_type":"markdown","source":"## Imports and settings\n\nAs always, this begins with imports. We sen set some environment variable to easily acces Google Cloud Storage anonymously.","metadata":{}},{"id":"f4c259a5-f645-48f9-ba76-e83bce896159","cell_type":"code","source":"import gcsfs\nimport rioxarray\nimport rasterio\nimport os\nimport numpy as np\nfrom distributed import Client\nimport xarray as xr\nimport pandas as pd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"872af06f-3098-474c-8d25-950aa2281b21","cell_type":"code","source":"def set_env():\n    os.environ[\"GS_NO_SIGN_REQUEST\"] = \"YES\"\n\nset_env()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0450ef8b-1836-43e6-9884-25e0420b5819","cell_type":"markdown","source":"## Check you have access to GCS bucket\n\nWe can use gcsfs package, which mimics a file system usage on an object storage.\n\nIt is pretty easy on a public bucket with anonymous access authorized, like the one we use. You can see that an object store with fsspec can behave a lot like a standard fil system. Just don't forget it is not.","metadata":{}},{"id":"77180241-340b-4da2-af6f-79a21e6ba230","cell_type":"code","source":"import gcsfs\nfs = gcsfs.GCSFileSystem(bucket_name=\"supaero\", token='anon')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1a03a66d-f5c1-46ae-bc49-30f78932d090","cell_type":"code","source":"fs.ls('supaero/31TCH')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"74b1b94a-0ef8-4699-b426-d4214f1eed7a","cell_type":"code","source":"fs.ls('supaero/31TCH/SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c46b4529-571e-4965-9a16-16072fbbe322","cell_type":"markdown","source":"### The products\n\nAs you can see, we'll use Sentinel-2 L2A products, which have been create using Maja algorithm from CNES.\nDo not hesitate to check Maja or [Theia website](https://www.theia-land.fr/en/product/sentinel-2-surface-reflectance/) to know more about these.\n\nFor example :\n\n- How many bands a product has?\n- What correction to the L1C product does MAJA makes?\n- Have every band the same resolution (this is important for the following)?","metadata":{}},{"id":"1a711fc6-bdd6-401a-b0c1-8678c4155485","cell_type":"markdown","source":"## Reading a product band using rioxarray and rasterio\n\nRasterio is the Pythonic interface to GDAL, the go to tool when dealing with satelite products. Rioxarray is a library that makes it easy to read rasterio compatible products as Xarray DataArray or Datasets.\n\nAt first, we'll read only a subsection of a product, using classical Numpy slices. This way, only selected pixels will be loaded into memory (important when using Dask afterwards, and to not blow up your computer or server memory).\n\nJust use the rioxarray.open_rasterio function to open a random tif file from above, then slice it in order to keep x and y dimension from pixels 4000 to 5000.\nWee'll have use an URL like \"gs://bucket/path\".\n\nOpening the file should be pretty fast. Then plot the dataset in another cell, data should really be accessed at this point.","metadata":{}},{"id":"db657988-8180-4b97-b64c-9df16c4546e5","cell_type":"code","source":"%%time\nswir = rioxarray.open_rasterio(\"gs://supaero/31TCH/SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2/SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2_FRE_B11.tif\")\nswir = swir[:,4000:5000,4000:5000]\nswir","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4a30d211-0a44-439c-bccc-18929ee99141","cell_type":"code","source":"%%time\nswir.plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"36cfb25f-c983-4894-b9c8-0b5a42e2ed56","cell_type":"markdown","source":"Even with the swir band, we can see we are in mountainous area.","metadata":{}},{"id":"ee90af8e-609d-4512-bce8-5b4518ee9cbe","cell_type":"markdown","source":"## Building a single date Dataset\n\nFirst, without using Dask, will build an Xarray Dataset from two DataArray that share the same dimensions and coordinates.\n\nAs we'll read two bands that to not share the same resolution, we'll have to resample the green band (using a simple mean).\n\nThen, we'll use the Dataset to compute two more variable, NDSI, and snow mask.\n\n### Build DataArrays\n\nSo first, we'll read the swir band as from above, but this time, we'll need to make sure that we handle the nodata value correctly (this will be important in the next part of this exercise). To do so, as these L2A products are note really standard, we need to manually remove the no data values using a filter. Then write it in rioxarray metadata.\n\nSo open the swir band as above, slice it the same way, but then user xarray.where to remove nodata values (-10_000), and use rioxarray write_no_data with encoded and inplace kwargs to set it correctly inside the data array.","metadata":{}},{"id":"7a24789b-2872-45ec-b8e6-a9294aa7caee","cell_type":"code","source":"swir = rioxarray.open_rasterio(\"gs://supaero/31TCH/SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2/SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2_FRE_B11.tif\")\nswir = swir[:,4000:5000,4000:5000]\n#No data handling\nswir = swir.where(swir != -10000)\nswir.rio.write_nodata(-10000, encoded=True, inplace=True)\nswir","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b8bb94d0-42ae-4094-8361-b46f101ae303","cell_type":"code","source":"swir.plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6467fa4e-3c99-49f6-bf23-59b76bcf4aff","cell_type":"markdown","source":"Note that Xarray with numpy backend is already a bit lazy:\n- If you only open a file, only its metadata are read, nothin is loaded until an operation like plot or load is performed.\n- When applying a metadata filter, we transform and load the data, how can you see it in the HTML representation of the DataArray in the notebook?","metadata":{}},{"id":"4ca4c09a-d28b-4293-8734-1384d65240cf","cell_type":"markdown","source":"Now, we need to open the green band. Note that its resolution is two times better on spatial dimension than the SWIR band.\n\nIn order to build a dataset or perform operations, we need to resample the data to the same resolution as the SWIR band.\n\nThere are several ways to do this, including reproject from rioxarray, but since here we only want to divide resolution by exactly two, we'll use a simple mean through the coarsen function of Xarray.\n\nSo to sum up, in the following cell you'll need to:\n- Open the green band of the same product as above,\n- Slice the data before reading it, we only want to load only the data of interest. Be carefull, we've got twice as many pixels at the beginning, so be sure to take the correct amount of pixels, and at the correct indices.\n- Apply coarsen to the result, with x=2, y=2, boundary='pad', and take the mean.\n- As above, filter nodata values.","metadata":{}},{"id":"fa28dc29-828d-4cdf-999c-e0cba00afd80","cell_type":"code","source":"green = rioxarray.open_rasterio(\"gs://supaero/31TCH/SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2/SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2_FRE_B3.tif\")\ngreen = green[:,8000:10000,8000:10000]\n# Rééchantillonage à 20m, diviser résolution par 2\ngreen = green.coarsen(x=2, y=2, boundary='pad').mean()\n#No data\ngreen = green.where(green != -10000)\ngreen.rio.write_nodata(-10000, encoded=True, inplace=True)\ngreen","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"eb5594cc-3967-4d52-9784-51b4184b47c7","cell_type":"markdown","source":"Then plot the DataArray you just created.\n\nYou should observe the same mountains, but this time, with the green band, you should see some snow!","metadata":{}},{"id":"b856133b-fd0d-4297-b3d5-d5e24d340c2d","cell_type":"code","source":"green.plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7fd414ca-ad69-4230-8203-4d916aff9f5f","cell_type":"markdown","source":"### Compute the NDSI using DataArrays\n\nSince the two DataArrays are of the same dimension, you can already perform operations with both of them.\n\nUse them two compute the [Normalised Difference Snow Index (NDSI)](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/ndsi/).\n\nThis should be as straigthforward as with Numpy.","metadata":{}},{"id":"7c1bf750-7cea-47d4-8aa5-dda6b5e8b45c","cell_type":"code","source":"ndsi = (green - swir) / (green + swir)\nndsi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a622754f-28b0-433c-8cae-500d34f7b6f3","cell_type":"markdown","source":"Did you tried to plot it?","metadata":{}},{"id":"9cf254c8-c0ad-450f-ad50-e8f34b37f8a4","cell_type":"code","source":"ndsi.plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7e62c9f2-1dd6-41c2-805f-392d77e177c8","cell_type":"markdown","source":"It has been established by research that a level of 0.4 in NDSI usually means there is snow.\n\nJust use this threshold to compute a boolean DataArray representing the snow cover over this particular area, and plot it. There should be plenty of snow on your image.","metadata":{}},{"id":"b514108d-5b43-405c-bf0f-45b6351f6409","cell_type":"code","source":"(ndsi > 0.4).plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d0712d3b-8833-4abd-92b7-8f721e9c6686","cell_type":"markdown","source":"### Xarray Datasets\n\nNow, we'll just use what we've done above to build a Simple Dataset made of several variables, because this will be handy in the next part of the exercise.\n\nBuilding a Dataset is as simple as using its constructor (xr.Dataset()) and giving it a dict with a string name associated to a DataArray.\n\nBuild a dataset with a green and swir variable pointing to the DataArray we built above, then display it to see its structure. You can also play with the Database buttons or metadata to have more informations.","metadata":{}},{"id":"18a82bd0-d2aa-48fb-b915-d2b72ed17ccc","cell_type":"code","source":"sub_ds = xr.Dataset({\"green\": green, \"swir\": swir})\nsub_ds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d95c5a4f-b5c9-4060-a42f-eeb9fdfc0f21","cell_type":"markdown","source":"A nice feature about Dataset is to be able to build and store new variables. We'll see also in the next part that we can add Temporal index as dimensions.\n\nFor now, we'll just create two new variable in this Dataset:\n- a ndsi variable, that is linked to the NDSI computation over the two other variables,\n- a snow variable, boolean array as above.\n\nAdding variable is a simple as affecting a new Column in pandas, e.g. dataset[\"ndsi\"] = ...","metadata":{}},{"id":"a7b969e5-6607-4fbd-a858-05edff3bde9b","cell_type":"code","source":"sub_ds[\"ndsi\"] = (sub_ds.green - sub_ds.swir) / (sub_ds.green + sub_ds.swir)\nsub_ds[\"snow\"] = sub_ds.ndsi > 0.4\nsub_ds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f17fd86e-e244-416a-8caf-57a904f13e1e","cell_type":"markdown","source":"Then plot the snow variable, just to check:","metadata":{}},{"id":"56f01a4d-4c37-482b-854d-286fdacc8afc","cell_type":"code","source":"sub_ds.snow.plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2366bc14-6e59-445d-aafe-b55100358e06","cell_type":"markdown","source":"### Compute snow percentage over the area\n\nThe end goalg below is to plot the evolution of snow cover accross a time serie. But how to plot a snow cover percentage over one product?\n\nYou'll need to count pixels having snow, and divide this number by the number of pixels wich are not nodata.","metadata":{}},{"id":"ae6759a9-bf78-4a1b-be38-699803553172","cell_type":"code","source":"sub_ds.snow.sum() / sub_ds.snow.count()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d6ba935e-5633-4566-b74b-0edf70c63284","cell_type":"markdown","source":"On this particular patch, you should get over 77% of snow cover.","metadata":{}},{"id":"62d5067a-6467-4030-ae12-fcc92661f029","cell_type":"markdown","source":"## Compute a time series analysis\n\nNow that we know how to create a single timestamp dataset, we'll build a dataset with a time dimension.\nThe idea is to stack single temporal datasets into a single one using a new time dimension.\n\nUp to now, we've only built datasets that easily fit in memory, by taking only part of one observation. \nIn order to be able to work on full images and on ten products, we'll need to use Dask.\n\n### Start a Dask cluster\n\nConfigure it according to your computing power. Then we'll need to set environment variable on every worker.","metadata":{}},{"id":"d840c368-e1b5-46e3-856b-c507fcff3160","cell_type":"code","source":"client = Client(n_workers=2, threads_per_worker=2, memory_limit='1GiB')\nclient","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8b486824-5a73-4fb3-9869-aec6f0c29baf","cell_type":"code","source":"client.run(set_env)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f9eb353a-95b2-47db-8763-ec8e1a488967","cell_type":"markdown","source":"It's really interesting when using Dask to start a real Distributed cluster (even on one machine like we've done above).\n\nThis gives access to a nice Dashboard, just click on the link displayed above (ending with 8787/status). If you are using binder, you should replace the URL with something like:\n\nhttps://hub.2i2c.mybinder.org/user/guillaumeeb-supaero-otsu-course-4kfqus3j/proxy/8787/status\n\nFirst part of the URL should be copied from you Jupyterlab URL on your browser.","metadata":{}},{"id":"32a17ae2-dc0b-4d28-a726-9ba41a6cff0d","cell_type":"markdown","source":"### Define some functions to build Datasets\n\nOK, in order to simplify the building of our time serie, we'll define some functions for the following:\n- Reading one band of a product. It should also handle an optional resampling. This time, we also want to use a Dask backend.\n- Creating a single time Dataset with green and swir band.\n\nSo first, we'll define a read_one_band(product, band, coarsen=1) function:\n- product is the name of the product, i.e. the \"SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2\" part, which is repeated in a folder, and in the file name.\n- band is the band identifier, i.e. B3 or B11 in our case. We always access \"FRE\" bands in our case.\n- coarsen is the level of subsambling, it should be 1 by default (no subsampling) for the SWIR band, set it to 2 for GREEN to perform a resampling.\n- When opening the file with rioxarray, we'll use two new arguments: chunks, and lock=False. chunks indicate that we want chunked array as a backend, so Dask Arrays. You use (-1, 1024 * coarsen, 1024*coarsen) as a value before opening the file.\n- We also want to remove the band dimension that is useless, in order to do so, just apply .squeeze('band', drop=True) right after the opening.","metadata":{}},{"id":"be1d021c-14f5-4512-abf6-80049959ad9f","cell_type":"code","source":"def read_one_band(product, band, coarsen=1):\n    chunks=(-1, 1024*coarsen, 1024*coarsen)\n    band = rioxarray.open_rasterio(f\"gs://supaero/31TCH/{product}/{product}_FRE_{band}.tif\", \n                                chunks=chunks,\n                                lock=False).squeeze('band', drop=True)\n    band = band.where(band != -10000)\n    band.rio.write_nodata(-10000, encoded=True, inplace=True)\n    if coarsen > 1:\n        band = band.coarsen(x=coarsen, y=coarsen, boundary='pad').mean()\n    return band","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"83aebc25-9e13-43a5-a88f-04aa4f63d833","cell_type":"markdown","source":"Try this function on the green band of \"SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2\", so with a coarsen=2 value, and whatch the HTML repr. What do you notice?","metadata":{}},{"id":"6b9766be-5cd7-415b-9462-808c999174ea","cell_type":"code","source":"read_one_band(\"SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2\", \"B3\", 2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ebfcfc33-e2a3-43dd-a919-72342b6b98f6","cell_type":"markdown","source":"Now, we'll define a create_dataset(product) function.\n\nProduct argument is the same as above, and it will create a 2 dimension dataset with two DataArray variables: green and swir, at the same resolution.","metadata":{}},{"id":"d82417c7-aa06-49cd-b459-690da9939e97","cell_type":"code","source":"def create_dataset(product):\n    ds = xr.Dataset({\"green\": read_one_band(product, \"B3\", 2), \"swir\": read_one_band(product, \"B11\")})\n    return ds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"880d5b12-9a6f-4d27-b0d6-eb1b1bdc3b07","cell_type":"markdown","source":"Try it on the above product","metadata":{}},{"id":"4c427ab7-efb0-4a50-bba0-4ece007d1245","cell_type":"code","source":"create_dataset(\"SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bf34c381-ab46-4e2d-8c53-af0dd637aef4","cell_type":"markdown","source":"### Build our time serie\n\nWe are now ready to build a time series of our ten products.\n\nIn order to do that, we'll need:\n- our product list (which is built in the below cell)\n- A list of containing all of our datasets\n- A Pandas Datetime index corresponding to our product list (code also given)\n- Apply xarray concat on our dataset list associated with our new Index as a dimension.","metadata":{}},{"id":"ac8faa01-2ba7-4691-8f09-0cd299300a52","cell_type":"code","source":"product_list = [path.split(\"/\")[-1] for path in fs.ls('supaero/31TCH')]\nproduct_list","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"edba060c-f99f-4438-8d9d-a6c63dd87a6f","cell_type":"code","source":"# Create time index\ndates = [product.split(\"_\")[1] for product in product_list]\ndt_index = pd.to_datetime(dates, format=\"%Y%m%d-%H%M%S-%f\")\ndt_index.name = \"time\"\ndt_index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e63dbaa7-a52b-48a5-bd98-c105bbb84c66","cell_type":"markdown","source":"So now, create a list by creating a dataset from all of the products above.\n\nThen, use xr.concat to create a new overall dataset with a new time dimension and all of our single time step datasets in it.","metadata":{}},{"id":"87fb6d81-300d-4da7-b145-898188e31001","cell_type":"code","source":"datasets = []\nfor product in product_list:\n    datasets.append(create_dataset(product))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9a8e20ab-36e8-4502-8b19-4b3938455b8d","cell_type":"code","source":"complete_ds = xr.concat(datasets, dt_index)\ncomplete_ds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"28ae2100-1d94-4582-8244-08726d9981f8","cell_type":"markdown","source":"### Add NDSI and snow mask\n\nJust as with a single time step Dataset, you can easily add two new variables to this complete Dataset as above.\n\nSo create the new ndsi and snow variables, as above.","metadata":{}},{"id":"010dc1dd-48a2-494a-b2d6-6b930d4dad10","cell_type":"code","source":"complete_ds[\"ndsi\"] = (complete_ds.green - complete_ds.swir) / (complete_ds.green + complete_ds.swir)\ncomplete_ds[\"snow\"] = complete_ds.ndsi > 0.4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"661404a7-2d87-48a0-81b7-1ebdb70506b9","cell_type":"markdown","source":"### Compute snow percentage time series over the whole image\n\nDid you notice?\n\nAll the operation above where pretty fast, no? That's because we didn't really computed anything. By using Dask backend and Xarray, we only worked using product metadata, all the real values are still waiting to be computed. This is the lazy power of Dask with Xarray.\n\nBut now, if we want some result, we'll need to trigger a computation.\n\nWhat we are interested in is a time series representing the snow percentage evolution over the ten dates.\n\nWe'll need to count snow pixel and sum them over spatial dimension, count also the not nodata values over each time step on the spatial dimension, and just divide the two time series to get the percentage.\n\nIn order to trigger the computation, we'll need to use compute method.\n\nThen whatch the Daks Dashboard. This should take some time, depending on your resources (on Binder it does).","metadata":{}},{"id":"54285c92-8b41-4380-b570-adf21aaa112b","cell_type":"code","source":"%%time\nsnow_percentage = (complete_ds.snow.sum(dim=[\"x\", \"y\"]) / complete_ds.ndsi.count(dim=[\"x\", \"y\"])).compute()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a4839e8b-6dbd-4275-835e-fc17b2676ea8","cell_type":"code","source":"snow_percentage","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"84dfa083-6bb6-4e65-9420-3cc1b0ae0d52","cell_type":"markdown","source":"### And what is the result?\n\nNow, we just want to plot the results.\n\nAs we've not sorted our dataset by dates at the beginning, you'll want to do it first, using xarray sortby method.\n\nThen just plot, or better, plot.step(where=\"mid\") for a nicer view (in my opinion).\n\nDoes this snow cover percentage time serie looks right?","metadata":{}},{"id":"522418ac-3cc4-4211-b5b0-26d2cc087b46","cell_type":"code","source":"snow_percentage.sortby(\"time\").plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"12d66943-2509-4793-ab9b-e66cce822b91","cell_type":"code","source":"snow_percentage.sortby(\"time\").plot.step(where=\"mid\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d21e7348-0209-4fa4-bb42-c8461be313d1","cell_type":"markdown","source":"# Extends other statistics\n\n- Snow area\n- NDSI\n- Add elevation, and compute snowline","metadata":{}},{"id":"69f89ad3-556a-4ac7-8666-8ebcc578c4bc","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}